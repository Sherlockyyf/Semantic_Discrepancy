{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2596d4-8b51-4562-a742-199a58e9a7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yfyuan/anaconda3/envs/semantic/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import cv2\n",
    "import torch\n",
    "import einops\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_lightning import seed_everything\n",
    "import torchvision.transforms as transformers\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0275d592-d291-4a30-89b3-22fa57fd68cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from ControlNet.cldm.model import create_model,load_state_dict\n",
    "from ControlNet.annotator.canny import CannyDetector\n",
    "from ControlNet.annotator.util import HWC3\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from src.controller import AttentionControl\n",
    "from src.ddim_v_hacked import DDIMVSampler\n",
    "from src.img_util import find_flat_region, numpy2tensor\n",
    "\n",
    "eta = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e23b5f-8f69-45d6-b7a0-bce26db79413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'Stable Diffusion 1.5': '',\n",
    "    'revAnimated_v11': 'models/revAnimated_v11.safetensors',\n",
    "    'realisticVisionV20_v20': 'models/realisticVisionV20_v20.safetensors',\n",
    "    'DGSpitzer/Cyberpunk-Anime-Diffusion': '~/YYF/all_models/Cyberpunk-Anime-Diffusion.safetensors',\n",
    "    'wavymulder/Analog-Diffusion': 'analog-diffusion-1.0.safetensors',\n",
    "    'Fictiverse/Stable_Diffusion_PaperCut_Model': '/home/yfyuan/YYF/all_models/papercut_v1.ckpt',\n",
    "}\n",
    "local_model = ['Fictiverse/Stable_Diffusion_PaperCut_Model', 'wavymulder/Analog-Diffusion', 'DGSpitzer/Cyberpunk-Anime-Diffusion']\n",
    "class ProcessingState(Enum):\n",
    "    NULL = 0\n",
    "    FIRST_IMG = 1\n",
    "    KEY_IMGS = 2\n",
    "class GlobalState:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sd_model = None\n",
    "        self.ddim_v_sampler = None\n",
    "        self.detector_type = None\n",
    "        self.detector = None\n",
    "        self.controller = None\n",
    "        self.processing_state = ProcessingState.NULL\n",
    "\n",
    "    def update_controller(self, inner_strength, mask_period, cross_period,\n",
    "                          ada_period, warp_period):\n",
    "        self.controller = AttentionControl(inner_strength, mask_period,\n",
    "                                           cross_period, ada_period,\n",
    "                                           warp_period)\n",
    "\n",
    "    def update_sd_model(self, sd_model, control_type):\n",
    "        if sd_model == self.sd_model:\n",
    "            return\n",
    "        self.sd_model = sd_model\n",
    "        model = create_model('./ControlNet/models/cldm_v15.yaml').cpu()\n",
    "        if control_type == 'HED':\n",
    "            model.load_state_dict(\n",
    "                load_state_dict(huggingface_hub.hf_hub_download(\n",
    "                    'lllyasviel/ControlNet', './models/control_sd15_hed.pth'),\n",
    "                    location=device))\n",
    "        elif control_type == 'canny':\n",
    "            model.load_state_dict(\n",
    "                load_state_dict('./models/control_sd15_canny.pth',\n",
    "                                location=device))\n",
    "        elif control_type == 'depth':\n",
    "            model.load_state_dict(\n",
    "                load_state_dict(huggingface_hub.hf_hub_download(\n",
    "                    'lllyasviel/ControlNet', 'models/control_sd15_depth.pth'),\n",
    "                    location=device))\n",
    "\n",
    "        model.to(device)\n",
    "        sd_model_path = model_dict[sd_model]\n",
    "        if len(sd_model_path) > 0:\n",
    "            # check if sd_model is repo_id/name otherwise use global REPO_NAME\n",
    "            if sd_model.count('/') == 1:\n",
    "                repo_name = sd_model\n",
    "\n",
    "            model_ext = os.path.splitext(sd_model_path)[1]\n",
    "            if model_ext == '.safetensors':\n",
    "                model.load_state_dict(load_file(sd_model_path), strict=False)\n",
    "            elif model_ext == '.ckpt' or model_ext == '.pth':\n",
    "                model.load_state_dict(torch.load(sd_model_path)['state_dict'],\n",
    "                                      strict=False)\n",
    "\n",
    "        try:\n",
    "            model.first_stage_model.load_state_dict(torch.load('./models/vae-ft-mse-840000-ema-pruned.ckpt')['state_dict'],strict=False)\n",
    "        except Exception:\n",
    "            print('Warning: We suggest you download the fine-tuned VAE',\n",
    "                  'otherwise the generation quality will be degraded')\n",
    "\n",
    "        self.ddim_v_sampler = DDIMVSampler(model)\n",
    "\n",
    "    def clear_sd_model(self):\n",
    "        self.sd_model = None\n",
    "        self.ddim_v_sampler = None\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def update_detector(self, control_type, canny_low=100, canny_high=200):\n",
    "        if self.detector_type == control_type:\n",
    "            return\n",
    "        if control_type == 'HED':\n",
    "            self.detector = HEDdetector()\n",
    "        elif control_type == 'canny':\n",
    "            canny_detector = CannyDetector()\n",
    "            low_threshold = canny_low\n",
    "            high_threshold = canny_high\n",
    "\n",
    "            def apply_canny(x):\n",
    "                return canny_detector(x, low_threshold, high_threshold)\n",
    "\n",
    "            self.detector = apply_canny\n",
    "\n",
    "        elif control_type == 'depth':\n",
    "            midas = MidasDetector()\n",
    "\n",
    "            def apply_midas(x):\n",
    "                detected_map, _ = midas(x)\n",
    "                return detected_map\n",
    "\n",
    "            self.detector = apply_midas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150ffd7b-f0d8-4892-8df4-bfb60f7dbdf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_state = GlobalState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f5f260-d1b1-4494-939a-42db5ba604a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sd_model = 'revAnimated_v11'\n",
    "sd_model = 'realisticVisionV20_v20' #真实风格\n",
    "# sd_model = 'DGSpitzer/Cyberpunk-Anime-Diffusion' #赛博朋克风格\n",
    "# sd_model = 'wavymulder/Analog-Diffusion' # 人物传记风格\n",
    "# sd_model = 'Fictiverse/Stable_Diffusion_PaperCut_Model' # 剪纸风格\n",
    "control_type = 'canny'\n",
    "low_threshold = 50\n",
    "high_threshold = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a5a5438-d539-41ef-a29f-4ae565dc8d70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'logit_scale', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'text_projection.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model config from [./ControlNet/models/cldm_v15.yaml]\n",
      "Loaded state_dict from [./models/control_sd15_canny.pth]\n",
      "Warning: We suggest you download the fine-tuned VAE otherwise the generation quality will be degraded\n"
     ]
    }
   ],
   "source": [
    "global_state.update_sd_model(sd_model, control_type)\n",
    "global_state.update_controller(0,0,0,0,0)\n",
    "# global_state.update_controller(0,0,0,0,0)\n",
    "global_state.update_detector(control_type, low_threshold, high_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2feef235-bf93-40ac-a8e5-dbb5b20fa9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_feature_maps(blocks, i, feature_type=\"input_block\"):\n",
    "    block_idx = 0\n",
    "    for block in blocks:\n",
    "        if feature_type == \"input_block\":\n",
    "            if \"Downsample\" in str(type(block[0])) and block_idx == 6:\n",
    "                save_feature_map(block[0].down_output_feature,\n",
    "                                 f\"down_output_{block_idx}_time_{0}\")\n",
    "        elif feature_type == \"output_block\":\n",
    "            if \"ResBlock\" in str(type(block[0])) and block_idx == 5:\n",
    "                save_feature_map(block[0].out_layers_features,\n",
    "                                 f\"{feature_type}_{block_idx}_out_layers_features_time_{0}\")\n",
    "                \n",
    "        if feature_type == \"output_block\":\n",
    "            if len(block) > 1 and \"SpatialTransformer\" in str(type(block[1])):\n",
    "                save_feature_map(block[1].transformer_blocks[0].attn1.tmp_sim,\n",
    "                                 f\"attn_{block_idx}_frame_{0}\")\n",
    "        block_idx += 1\n",
    "\n",
    "def save_feature_maps_callback(i, unet_model):\n",
    "    save_feature_maps(unet_model.input_blocks, i, \"input_block\")\n",
    "    save_feature_maps(unet_model.output_blocks, i, \"output_block\")\n",
    "\n",
    "\n",
    "def save_feature_map(feature_map, filename):\n",
    "    os.makedirs(feature_maps_path, exist_ok=True)\n",
    "    print(f\"saving feature map in {feature_maps_path}\")\n",
    "    save_path = os.path.join(feature_maps_path, f\"{filename}.pt\")\n",
    "    torch.save(feature_map, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17105987-ca42-4b28-904f-20026b585884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def single_inversion(x0, ddim_v_sampler, img_callback=None):\n",
    "    # controller.cls = \"attn\"\n",
    "    model = ddim_v_sampler.model\n",
    "\n",
    "    prompt = f\"\"\n",
    "    cond = {\n",
    "        'c_concat': None,\n",
    "        'c_crossattn': [\n",
    "            model.get_learned_conditioning(\n",
    "                [prompt]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    un_cond = {\n",
    "        'c_concat': None,\n",
    "        'c_crossattn': [\n",
    "            model.get_learned_conditioning(\n",
    "                ['']\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    ddim_v_sampler.encode_ddim(x0, 1000, cond, un_cond, controller=None, img_callback=img_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c63d1138-fb7d-4141-a6b5-b1e433c0f370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "ddim_v_sampler = global_state.ddim_v_sampler\n",
    "model = ddim_v_sampler.model\n",
    "detector = global_state.detector\n",
    "controller = global_state.controller\n",
    "controller.set_task('')\n",
    "model.control_scales = [0.9] * 13\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "673d2c33-7c76-4a14-b16e-6dc588cc82c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### 批量出图  ##########\n",
    "first_strength = 1 - 0.95\n",
    "\n",
    "# processed = [dir_name[:-len(\"realisticVisionV20_v20_20\")-1] for dir_name in os.listdir(\"/home/yfyuan/YYF/Rerender/exp/ImageNet/results\")]\n",
    "\n",
    "prompt = f\"realistic photo\"\n",
    "# prompt = f\"in cartoon style\"\n",
    "a_prompt = \"RAW photo, subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\"\n",
    "n_prompt = \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation\"\n",
    "\n",
    "# prompt = \"((Papercut)), shapes, delicate patterns, silhouette, kirigami, sharp outline, Spread the picture\"\n",
    "# a_prompt = \"(masterpiece, top quality, best quality)\"\n",
    "# n_prompt = \"frame,decorations\"\n",
    "\n",
    "# prompt = \"in CG style\"\n",
    "# a_prompt = \"extremely detailed\"\n",
    "# n_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d34915c-bd42-40c9-a15f-8a38f778c414",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n",
      "saving feature map in /home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn/goldfish_inv\n"
     ]
    }
   ],
   "source": [
    "# 0 5 8 9\n",
    "seed_everything(0)\n",
    "from tqdm.notebook import tqdm\n",
    "num_samples = 1\n",
    "steps = 20\n",
    "unet_model = model.model.diffusion_model\n",
    "dir_ = \"goldfish\"\n",
    "img_name = \"painting_31.jpg\"\n",
    "img_dir = f\"/home/yfyuan/YYF/Rerender/exp/ImageNet/imagenet-r/{dir_}\"\n",
    "feature_maps_path = \"/home/yfyuan/YYF/Rerender/exp/attn_map/exp_attn\"\n",
    "feature_maps_path_denoising = os.path.join(feature_maps_path, f\"{dir_}\")\n",
    "feature_maps_path_inv = os.path.join(feature_maps_path, f\"{dir_}_inv\")\n",
    "\n",
    "controller.set_task('')\n",
    "img_path = os.path.join(img_dir, img_name)\n",
    "with torch.no_grad():\n",
    "    def ddim_sampler_callback(i):\n",
    "        save_feature_maps_callback(i, unet_model)\n",
    "    def generate_first_img(x0, img, strength):\n",
    "        samples, _ = ddim_v_sampler.sample(\n",
    "            steps,\n",
    "            num_samples,\n",
    "            shape,\n",
    "            cond,\n",
    "            verbose=False,\n",
    "            eta=eta,\n",
    "            unconditional_guidance_scale=7.5,\n",
    "            unconditional_conditioning=un_cond,\n",
    "            controller=controller,\n",
    "            x0=x0,\n",
    "            strength=strength,\n",
    "            img_callback=None)\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples_np = (\n",
    "                einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 +\n",
    "                127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "        return x_samples, x_samples_np\n",
    "\n",
    "    frame = cv2.imread(img_path)\n",
    "    shape = frame.shape\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = np.array(transformers.CenterCrop((frame.shape[0] // 64 * 64, frame.shape[1] // 64 * 64))(Image.fromarray(frame)))\n",
    "\n",
    "    img = HWC3(frame)\n",
    "    H, W, C = img.shape\n",
    "    shape = (4, H // 8, W // 8)\n",
    "    img_ = numpy2tensor(img)\n",
    "    \n",
    "    encoder_posterior = model.encode_first_stage(img_.to(device))\n",
    "    x0 = model.get_first_stage_encoding(encoder_posterior).detach()\n",
    "    \n",
    "    unet_model.unet_type = \"denoising\"\n",
    "    feature_maps_path = feature_maps_path_inv\n",
    "    # single_inversion(x0, ddim_v_sampler, ddim_sampler_callback)\n",
    "    \n",
    "    feature_maps_path = feature_maps_path_denoising\n",
    "    controller.set_task(['initfirst'])\n",
    "    controller.batch_frame_attn_feature_path = feature_maps_path\n",
    "    controller.threshold_block_idx = [3,4]\n",
    "\n",
    "    detected_map = detector(img)\n",
    "    detected_map = HWC3(detected_map)\n",
    "    control = torch.from_numpy(detected_map.copy()).float().to(device) / 255.0\n",
    "    control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "    control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
    "    cond = {\n",
    "        'c_concat': [control],\n",
    "        'c_crossattn': [\n",
    "            model.get_learned_conditioning(\n",
    "                [prompt + ', ' + a_prompt] * num_samples)\n",
    "        ]\n",
    "    }\n",
    "    un_cond = {\n",
    "        'c_concat': [control],\n",
    "        'c_crossattn':\n",
    "            [model.get_learned_conditioning([n_prompt] * num_samples)]\n",
    "    }\n",
    "\n",
    "    unet_model.unet_type = \"spatial\"\n",
    "\n",
    "    x_samples, x_samples_np = generate_first_img(x0, img, first_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae2c49-9b91-4d00-a679-54470b9a60b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image.fromarray(x_samples_np[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbd695f7-793a-4492-861d-7884dc3c3335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_num = f\"{controller.threshold_block_idx[0]}-{controller.threshold_block_idx[-1]}\"\n",
    "Image.fromarray(x_samples_np[0]).save(f\"/home/yfyuan/YYF/Rerender/exp/exp_all_attn/{dir_}_{steps}_{block_num}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7ca07-6902-4bc5-ab73-524c786c8fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic",
   "language": "python",
   "name": "semantic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
