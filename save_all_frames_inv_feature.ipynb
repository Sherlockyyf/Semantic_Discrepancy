{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9125dd-dd6b-4f2a-8e01-fb1e5b212aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import cv2\n",
    "import torch\n",
    "import einops\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_lightning import seed_everything\n",
    "import torchvision.transforms as transformers\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6788d45d-1c9f-43a7-8242-28fcda86f3af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from ControlNet.cldm.model import create_model,load_state_dict\n",
    "from ControlNet.annotator.canny import CannyDetector\n",
    "from ControlNet.annotator.util import HWC3\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from src.controller import AttentionControl\n",
    "from src.ddim_v_hacked import DDIMVSampler\n",
    "from src.img_util import find_flat_region, numpy2tensor\n",
    "\n",
    "import huggingface_hub\n",
    "REPO_NAME = 'Anonymous-sub/Rerender'\n",
    "eta = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ab38b8-cb24-4be4-9aaf-e6ab58f154c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'Stable Diffusion 1.5': '',\n",
    "    'revAnimated_v11': 'models/revAnimated_v11.safetensors',\n",
    "    'realisticVisionV20_v20': 'models/realisticVisionV20_v20.safetensors',\n",
    "    'DGSpitzer/Cyberpunk-Anime-Diffusion': 'Cyberpunk-Anime-Diffusion.safetensors',\n",
    "    'wavymulder/Analog-Diffusion': 'analog-diffusion-1.0.safetensors',\n",
    "    'Fictiverse/Stable_Diffusion_PaperCut_Model': 'papercut_v1.ckpt',\n",
    "}\n",
    "local_model = ['Fictiverse/Stable_Diffusion_PaperCut_Model', 'wavymulder/Analog-Diffusion', 'DGSpitzer/Cyberpunk-Anime-Diffusion']\n",
    "class ProcessingState(Enum):\n",
    "    NULL = 0\n",
    "    FIRST_IMG = 1\n",
    "    KEY_IMGS = 2\n",
    "class GlobalState:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sd_model = None\n",
    "        self.ddim_v_sampler = None\n",
    "        self.detector_type = None\n",
    "        self.detector = None\n",
    "        self.controller = None\n",
    "        self.processing_state = ProcessingState.NULL\n",
    "\n",
    "    def update_controller(self, inner_strength, mask_period, cross_period,\n",
    "                          ada_period, warp_period):\n",
    "        self.controller = AttentionControl(inner_strength, mask_period,\n",
    "                                           cross_period, ada_period,\n",
    "                                           warp_period)\n",
    "\n",
    "    def update_sd_model(self, sd_model, control_type):\n",
    "        if sd_model == self.sd_model:\n",
    "            return\n",
    "        self.sd_model = sd_model\n",
    "        model = create_model('./ControlNet/models/cldm_v15.yaml').cpu()\n",
    "        if control_type == 'HED':\n",
    "            model.load_state_dict(\n",
    "                load_state_dict(huggingface_hub.hf_hub_download(\n",
    "                    'lllyasviel/ControlNet', './models/control_sd15_hed.pth'),\n",
    "                    location=device))\n",
    "        elif control_type == 'canny':\n",
    "            model.load_state_dict(\n",
    "                load_state_dict(huggingface_hub.hf_hub_download(\n",
    "                    'lllyasviel/ControlNet', 'models/control_sd15_canny.pth'),\n",
    "                    location=device))\n",
    "        elif control_type == 'depth':\n",
    "            model.load_state_dict(\n",
    "                load_state_dict(huggingface_hub.hf_hub_download(\n",
    "                    'lllyasviel/ControlNet', 'models/control_sd15_depth.pth'),\n",
    "                    location=device))\n",
    "\n",
    "        model.to(device)\n",
    "        sd_model_path = model_dict[sd_model]\n",
    "        if len(sd_model_path) > 0:\n",
    "            repo_name = REPO_NAME\n",
    "            # check if sd_model is repo_id/name otherwise use global REPO_NAME\n",
    "            if sd_model.count('/') == 1:\n",
    "                repo_name = sd_model\n",
    "\n",
    "            model_ext = os.path.splitext(sd_model_path)[1]\n",
    "            if sd_model in local_model:\n",
    "                downloaded_model = os.path.join(\"/home/yfyuan/YYF/all_models\", model_dict[sd_model]) \n",
    "            else:\n",
    "                downloaded_model = huggingface_hub.hf_hub_download(\n",
    "                    repo_name, sd_model_path)\n",
    "            if model_ext == '.safetensors':\n",
    "                model.load_state_dict(load_file(downloaded_model),\n",
    "                                      strict=False)\n",
    "            elif model_ext == '.ckpt' or model_ext == '.pth':\n",
    "                model.load_state_dict(\n",
    "                    torch.load(downloaded_model)['state_dict'], strict=False)\n",
    "\n",
    "        try:\n",
    "            model.first_stage_model.load_state_dict(torch.load(\n",
    "                huggingface_hub.hf_hub_download(\n",
    "                    'stabilityai/sd-vae-ft-mse-original',\n",
    "                    'vae-ft-mse-840000-ema-pruned.ckpt'))['state_dict'],\n",
    "                strict=False)\n",
    "        except Exception:\n",
    "            print('Warning: We suggest you download the fine-tuned VAE',\n",
    "                  'otherwise the generation quality will be degraded')\n",
    "\n",
    "        self.ddim_v_sampler = DDIMVSampler(model)\n",
    "\n",
    "    def clear_sd_model(self):\n",
    "        self.sd_model = None\n",
    "        self.ddim_v_sampler = None\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def update_detector(self, control_type, canny_low=100, canny_high=200):\n",
    "        if self.detector_type == control_type:\n",
    "            return\n",
    "        if control_type == 'HED':\n",
    "            self.detector = HEDdetector()\n",
    "        elif control_type == 'canny':\n",
    "            canny_detector = CannyDetector()\n",
    "            low_threshold = canny_low\n",
    "            high_threshold = canny_high\n",
    "\n",
    "            def apply_canny(x):\n",
    "                return canny_detector(x, low_threshold, high_threshold)\n",
    "\n",
    "            self.detector = apply_canny\n",
    "\n",
    "        elif control_type == 'depth':\n",
    "            midas = MidasDetector()\n",
    "\n",
    "            def apply_midas(x):\n",
    "                detected_map, _ = midas(x)\n",
    "                return detected_map\n",
    "\n",
    "            self.detector = apply_midas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67e63b3-3d8a-48d9-b6ff-81f7db27bbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_state = GlobalState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9522b8-dcc2-4706-8bdb-8625c9283ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sd_model = 'revAnimated_v11'\n",
    "# sd_model = 'realisticVisionV20_v20' #真实风格\n",
    "# sd_model = 'DGSpitzer/Cyberpunk-Anime-Diffusion' #赛博朋克风格\n",
    "# sd_model = 'wavymulder/Analog-Diffusion' # 人物传记风格\n",
    "# sd_model = 'Fictiverse/Stable_Diffusion_PaperCut_Model' # 剪纸风格\n",
    "control_type = 'canny'\n",
    "low_threshold = 50\n",
    "high_threshold = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18db718-d1f4-480a-a227-16c128f361ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'text_projection.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'logit_scale', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model config from [./ControlNet/models/cldm_v15.yaml]\n",
      "Loaded state_dict from [/home/yfyuan/.cache/huggingface/hub/models--lllyasviel--ControlNet/snapshots/e78a8c4a5052a238198043ee5c0cb44e22abb9f7/models/control_sd15_canny.pth]\n"
     ]
    }
   ],
   "source": [
    "global_state.update_sd_model(sd_model, control_type)\n",
    "global_state.update_controller(0,0,0,0,0)\n",
    "global_state.update_detector(control_type, low_threshold, high_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dcd5cf5-6ca0-4c81-8b79-342bc95e1d82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "ddim_v_sampler = global_state.ddim_v_sampler\n",
    "model = ddim_v_sampler.model\n",
    "detector = global_state.detector\n",
    "controller = global_state.controller\n",
    "model.control_scales = [0.9] * 13\n",
    "model.to(device)\n",
    "\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68b006c-0848-4b24-87cc-e0bd709fdfb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_model = model.model.diffusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f61695-db0b-4559-99fa-6affcfc096a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_maps_path = \"./exp/attn_map/all_frames_inv_features\"\n",
    "os.makedirs(feature_maps_path, exist_ok=True)\n",
    "def save_feature_map(feature_map, filename):\n",
    "    save_path = os.path.join(feature_maps_path, f\"{filename}.pt\")\n",
    "    torch.save(feature_map, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f79be1c4-5fe5-4fbf-9561-e26612e8efbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./result/pexels-koolshooters-7322716/video/0000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Inversion:   0%|                                                                                                                                                    | 0/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "frames_path = \"./result/pexels-koolshooters-7322716/video\"\n",
    "frames = sorted(os.listdir(frames_path))\n",
    "for idx, frame in enumerate(frames):\n",
    "    img_path = os.path.join(frames_path, frame)\n",
    "    print(img_path)\n",
    "    frame = cv2.imread(img_path)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = HWC3(frame)\n",
    "    H, W, C = img.shape\n",
    "\n",
    "    img_ = numpy2tensor(img)\n",
    "    encoder_posterior = model.encode_first_stage(img_.to(device))\n",
    "    x0 = model.get_first_stage_encoding(encoder_posterior).detach()\n",
    "    prompt = f\"\"\n",
    "    cond = {\n",
    "        'c_concat': None,\n",
    "        'c_crossattn': [\n",
    "            model.get_learned_conditioning(\n",
    "                [prompt]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    un_cond = {\n",
    "        'c_concat': None,\n",
    "        'c_crossattn': [\n",
    "            model.get_learned_conditioning(\n",
    "                ['']\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    def feature_callback(a,b,c):\n",
    "        block_idx = 0\n",
    "        # feature_type = \"output_block\"\n",
    "        feature_type = \"input_block\"\n",
    "        if feature_type == \"input_block\":\n",
    "            blocks = unet_model.input_blocks\n",
    "        elif feature_type == \"output_block\":\n",
    "            blocks = unet_model.output_blocks\n",
    "        for block in blocks:\n",
    "            if \"Downsample\" in str(type(block[0])):\n",
    "                save_feature_map(block[0].down_output_feature,\n",
    "                                 f\"down_output_{block_idx}_time_{0}\")\n",
    "            \n",
    "            # if \"ResBlock\" in str(type(block[0])):\n",
    "            #     save_feature_map(block[0].out_layers_features,\n",
    "            #                      f\"{feature_type}_{block_idx}_out_layers_features_time_{0}\")\n",
    "\n",
    "            # if len(block) > 1 and \"SpatialTransformer\" in str(type(block[1])):\n",
    "            #     save_feature_map(block[1].transformer_blocks[0].attn1.tmp_sim, f\"attn_{block_idx}_frame_{idx}\")\n",
    "            \n",
    "            # if len(block) > 1 and \"SpatialTransformer\" in str(type(block[1])):\n",
    "            #     save_feature_map(block[0].out_layers_features,\n",
    "            #                      f\"{feature_type}_{block_idx}_out_layers_features_time_{0}\")\n",
    "            \n",
    "            # if len(block) > 1 and \"SpatialTransformer\" in str(type(block[1])):\n",
    "            #     save_feature_map(block[1].attention_output, f\"attn_output_{block_idx}_frame_{idx}\")\n",
    " \n",
    "            block_idx += 1\n",
    "    \n",
    "    ddim_v_sampler.encode_ddim(x0, 1000, cond, un_cond, controller=None, img_callback=feature_callback)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81b3689-827b-4f03-8b63-d2c1ba02b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"/home/yfyuan/YYF/Rerender/exp/attn_map/all_frames_inv_features/attn_output_7_frame_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a80544-c309-4812-a6fd-ad62b195425c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280, 16, 18])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301cbc6-95a8-43a4-883d-22f3ea9899c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2v",
   "language": "python",
   "name": "v2v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
